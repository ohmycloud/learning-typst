#import "@preview/cetz:0.1.2"
#import "@preview/splash:0.3.0": *

#set page(columns: 2, margin: .5cm)
#let code(body) = {
  set text(weight: "regular")
  show: box.with(
    fill: luma(249),
    inset: 0.4em,
    radius: 3pt,
    baseline: 0.4em,
    width: 60%,
  )
  [#body]
}

#show "训练数据": it => [
    #text(fill: red)[#it]
]

#show "测试数据": it => [
    #text(fill: blue)[#it]
]

#show heading.where(level: 1): it => {
    align(left)[#code(text(tol-light.light-blue)[ *#it*])]
    v(0.25em)
}

#show heading.where(level: 2): it => {
    align(left)[#code(text(tol-light.light-blue)[ *#it*])]
    v(0.25em)
}

#outline()

= 术语

== 感知机

感知机(perceptron)接收多个输入信号，输出一个信号。这里所说的“信号”可以想象成电流那样具备“流动性”的东西。
像电流流过导线，向前方输送电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电流不同的是，感知机的信号只有“流/不流”(1/0)两种取值。
0 对应“不传递信号”，1对应“传递信号”。

#cetz.canvas({
    import cetz.draw: *
    set-style(
        circle: (
            fill: none,
            stroke: gray
        )
    )
    content((0,0),[$x_2$])
    circle((0,0), radius: (1, 1))
    content((1.5,1.5), [$w_2$])
    line((1,0), (3.0, 1.9), stroke: (paint: gray), mark: (end: ">", fill: none))
    content((0,4),[$x_1$])
    content((2.5,3.4), [$w_1$])
    circle((0,4), radius: (1, 1))
    line((1,4), (3.0, 1.9), stroke: (paint: gray), mark: (end: ">", fill: none))
    content((4.0, 1.9),[y])
    circle((4,2), radius: (1, 1))
})

上图是一个接收两个输入信号的感知机的例子。$x_1$、$x_2$ 是输入信号，y 是输出信号，_$w_1$_、_$w_2$_ 是权重(_w_ 是 weight 的首字母)。图中的 ○ 称为“神经元”或者“节点”。
输入信号被送往神经元时，会被分别乘以固定的权重(_$w_1x_1$_、_$w_2x_2$_)。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出 1。这也称为“神经元被激活”。这里将这个界限值称为#text(weight: "bold")[阈值]，用符号 θ 表示。

感知机的运行原理只有这些！把上述内容用数学公示来表示，就是式(2.1)。

$
 y = cases(
    0 （w_1x_1 + w_2x_2 <= θ）,
    1 （w_1x_1 + w_2x_2 > θ）
 )
$

感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。

== 与门

与门是有两个输入和一个输出的门电路。下图这种输入信号和输出信号的对应表称为“真值表”。与门仅在两个输入均为1时输出 1，其他时候则输出 0。

#table(
    columns: (1fr, 1fr, 1fr),
    inset: 5pt,
    align: center,
    stroke: (paint: gray, thickness: 1pt, dash: "dashed"),
    [$x_1$], [$x_2$], [y],
    [0], [0], [0],
    [1], [0], [0],
    [0], [1], [0],
    [1], [1], [1]
)

== 激活函数

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

```python
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    return exp_a / sum_exp_a
```

改进版：

```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c) # 溢出对策
    sum_exp_a = np.sum(exp_a)
    return exp_a / sum_exp_a
```

#let softmax = $ sum_(i=1)^n e^(a_i) $

$
y_k = e^(a_k) / #softmax
$

改进的 softmax 函数:

$
y_k = e^(a_k) / (sum_(i=1)^n e^(a_i)) 
 &= (C e^(a_k)) / (C sum_(i=1)^n e^(a_i)) \
 &= e^(a_k + log C) / (sum_(i=1)^n e^(a_i + log C)) \
 &= e^(a_k + C') / (sum_(i=1)^n e^(a_i + C'))
$

== 均方误差

均方误差(mean squared error) 的公示如下:

$
E = 1/2 sum_(k)(y_k - t_k)^2
$

这里, $y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据，k 表示数据的维数。

Python 代码实现如下:

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t) ** 2)
```

== 交叉熵误差

交叉熵误差(cross entropy)的公示如下:

$
E = - sum_(k) t_k log y_k
$

这里, log 表示以 e 为底数的自然对数($log _e$)。$y_k$ 是神经网络的输出，$t_k$ 是正确解标签。并且，$t_k$ 中只有正确解标签的索引为 1，其他均为 0(one-hot 表示)。因此，上面的公示实际上只计算对应正确解标签的输出的自然对数。

比如，假设 正确解标签的索引是“2”，与之对应的神经网络的输出是 0.6，则交叉熵误差 是 $−log 0.6$ = 0.51;若“2”对应的输出是 0.1，则交叉熵误差为 $−log 0.1$ = 2.30。 也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的。

Python 代码实现如下:

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

这里，参数 y 和 t 是 NumPy 数组。函数内部在计算 np.log 时，加上了一个微小的 delta。这是因为，当出现 np.log(0) 时, np.log(0) 会变为负无限大的 -inf，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。

== 损失函数

神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。神经网络的学习中所用的指标称为损失函数(loss function)。这个损失函数可以使用任意函数，但一般使用均方误差和交叉熵误差等。

损失函数是表示神经网络性能的“恶劣程序”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。

以“性能的恶劣程度”为指标可能会让人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情的本质上都是一样的。

== 特征量

“特征量”是指可以从输入数据(例如输入图像)中准确的提取本质数据(重要的数据)的转换器。
图像的特征量通常表示为向量的形式。在计算机视觉领域, 常用的特征量包括 SIFT、SURF 和 HOG 等。
使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的 SVM、KNN 等分类器进行学习。

ToDo: SIFT、SURF 和 HOG 分别是什么?

== 转换器

== 分类器

== 训练数据和测试数据

机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。

为什么需要将数据分为训练数据和测试数据呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能力，
就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。

泛化能力是指处理未被观察过的数据(不包含在训练数据中的数据)的能力。获得泛化能力是机器学习的最终目标。比如，在识别手写数字的问题中，泛化能力可能会被用在自动读取明信片的邮政编码的系统上。此时，手写数字识别就必须具备较高的识别“某个人”写的字的能力。注意这里不是“特定的某个人写的特定的文字”，而是“任意一个人写的任意文字”。

因此，仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。

== 过拟合和欠拟合

以手写数字识别为例，过拟合是指，虽然训练数据中的数字图像能被正确判别，但是不在训练数据中的数字图像却无法被识别的现象。

== epoch

*epoch* 是一个单位。一个 epoch 表示学习中所有训练数据均被使用过一次时的更新次数。

== 正向传播和反向传播

正向传播(forward propagation)
反向传播(backward propagation)

== 导数

导数就是表示某个瞬间的变化量。它可以定义成下面的式子。

$
(d f(x)) / (\dx) = op("lim", limits: #true)_(h->0) (f(x + h) - f(x)) / h
$

上面表示的是函数的导数。左边的符号 $(d f(x))/(\dx)$ 表示 $f(x)$ 关于 _x_ 的导数，即 $f(x)$ 相对于 _x_ 的变化程度。上面的式子表示的导数的含义是，_x_ 的“微小变化”将导致函数 f(x) 的值在多大程度上发生变化。

== 偏导数

在数学中，偏导数(partial derivative)的定义是：一个多变量的函数(或称多元函数)，对其中一个变量微分，而保持其他变量恒定。

函数 $f$ 关于变量 _x_ 的偏导数写为 $f'_x$ 或 $(∂f)/(∂x)$。偏导数符号 ∂ 是全导数符号 _d_ 的变体。

$f(x_0, x_1) = x_0^2 + x_1^2$

上面的公式有两个变量，所以有必要区分对哪个变量求导数，即对 $x_0$ 和 $x_1$ 两个变量中的哪一个求导数。

用数学公示表示为:
$(∂f)/(∂x_0)$、$(∂f)/(∂x_1)$、

== 梯度

像 ($(∂f)/(∂x_0)$, $(∂f)/(∂x_1)$) 这样的由全部变量的偏导数汇总而成的向量称为梯度(gradient)。

梯度指示的方向是各点处的函数值减少最多的方向。

== generative adversarial network

GAN 生成式对抗网络。

= 神经网络的学习

神经网络的“学习”是指从训练数据中自动获取最优权重参数的过程。学习的目的就是以损失函数为基准，找出能使它的值达到最小的权重参数。

mini-batch 学习

$
E = - 1 / N sum_(n) sum_(k) t_(\nk) log y_(\nk)
$